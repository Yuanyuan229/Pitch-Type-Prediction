{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pitch Data - Web Scrapping and Database creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "import requests\n",
    "import csv\n",
    "import pymysql\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b><font size=\"4\">Web Scraping</font></b></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the Batter_Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Creating a function to scrape Starting Batters Data\n",
    "def getStandardBatting(year):\n",
    "    csvfile = str(year) + '-standard-batting.csv'\n",
    "    if os.path.isfile(csvfile):\n",
    "        dd = pd.read_csv(csvfile)\n",
    "    else:\n",
    "        #request the different URLS based on function input\n",
    "        url = requests.get(\"https://www.baseball-reference.com/leagues/MLB/\"+str(year)+\"-standard-batting.shtml\")\n",
    "        #pull the HTML code\n",
    "        soup = BeautifulSoup(url.content, \"html.parser\")\n",
    "        #Find the table we want to scrap\n",
    "        div = soup.find('div', id='all_players_standard_batting')\n",
    "        #find the contents we want to scrap\n",
    "        bdiv = bytearray(str(div.contents),'utf-8')\n",
    "        #pull that contents into a soup object \n",
    "        soup2 = BeautifulSoup(bdiv, \"html.parser\")\n",
    "        tbody = soup2.find('tbody')\n",
    "        #find all the rows we want\n",
    "        rows = tbody.find_all('tr')\n",
    "        #label all the columns\n",
    "        columns = ['Name','Age','Tm']\n",
    "        #makes these the column names in the dataframe\n",
    "        dd = pd.DataFrame(columns = columns)\n",
    "        irow = 0\n",
    "        #run a for loop to get each row that exists\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            if (len(cols) >= 28):\n",
    "                dd.loc[irow] = [\n",
    "                    cols[0].text.strip(),\n",
    "                    cols[1].text.strip(),\n",
    "                    cols[2].text.strip()\n",
    "                ]\n",
    "                irow = irow+1\n",
    "        dd.index += 1\n",
    "         # Step 2: Sample of CSV being saved. Final Project will have all scraped data loaded in a mysql database\n",
    "        dd.to_csv(csvfile)\n",
    "    return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Clean the Data to Fit ERD\n",
    "def cleanStandardBatting(year):    \n",
    "    csvfile = str(year) + '-standard-batting-cleaned.csv'\n",
    "    if os.path.isfile(csvfile):\n",
    "        df = pd.read_csv(csvfile)\n",
    "    else:\n",
    "        df = pd.read_csv(str(year) + '-standard-batting.csv')\n",
    "        #remove unneeded columns\n",
    "        df = df.drop(columns=['Unnamed: 0', 'Age'])\n",
    "        ## Rename columns\n",
    "        df=df.rename(columns={\"Name\": \"batter_name\", \"Tm\": \"team\"})\n",
    "        df['year'] = str(year)\n",
    "        df.index += 1\n",
    "        df.to_csv(csvfile)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Run the Function to Get Data\n",
    "getStandardBatting(2015)\n",
    "getStandardBatting(2016)\n",
    "getStandardBatting(2017)\n",
    "getStandardBatting(2018)\n",
    "\n",
    "## Run the Fnciton to Clean Data \n",
    "batters_2015 = cleanStandardBatting(2015)\n",
    "batters_2016 = cleanStandardBatting(2016)\n",
    "batters_2017 = cleanStandardBatting(2017)\n",
    "batters_2018 = cleanStandardBatting(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge Data \n",
    "batter_stats = pd.concat([batters_2015,batters_2016,batters_2017,batters_2018],sort=False)\n",
    "batter_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = batter_stats.drop(columns=['Unnamed: 0'])\n",
    "df.to_csv('batter_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the Pitcher_Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Creating a function to scrape Starting Pitcher Data\n",
    "def getStandardPitching(year):\n",
    "    csvfile = str(year) + '-standard-pitching.csv'\n",
    "    if os.path.isfile(csvfile):\n",
    "        dd = pd.read_csv(csvfile)\n",
    "    else:\n",
    "        #request the different URLS based on function input\n",
    "        url = requests.get(\"https://www.baseball-reference.com/leagues/MLB/\"+str(year)+\"-standard-pitching.shtml\")\n",
    "        #pull the HTML code\n",
    "        soup = BeautifulSoup(url.content, \"html.parser\")\n",
    "        #Find the table we want to scrap\n",
    "        div = soup.find('div', id='all_players_standard_pitching')\n",
    "        #find the contents we want to scrap\n",
    "        bdiv = bytearray(str(div.contents),'utf-8')\n",
    "        #pull that contents into a soup object \n",
    "        soup2 = BeautifulSoup(bdiv, \"html.parser\")\n",
    "        tbody = soup2.find('tbody')\n",
    "        #find all the rows we want\n",
    "        rows = tbody.find_all('tr')\n",
    "        #label all the columns\n",
    "        columns = ['Name','Age','Tm','Lg','W','L','W-L%','ERA','G','GS','GF','CG','SHO','SV','IP','H',\n",
    "                   'R','ER','HR','BB','IBB','SO','HBP','BK','WP','BF','ERA+','FIP','WHIP','H9',\n",
    "                   'BB9','SO9','SO/W']\n",
    "        #makes these the column names in the dataframe\n",
    "        dd = pd.DataFrame(columns = columns)\n",
    "        irow = 0\n",
    "        #run a for loop to get each row that exists\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            if (len(cols) >= 28):\n",
    "                dd.loc[irow] = [\n",
    "                    cols[0].text.strip(),\n",
    "                    cols[1].text.strip(),\n",
    "                    cols[2].text.strip(),\n",
    "                    cols[3].text.strip(),\n",
    "                    cols[4].text.strip(),\n",
    "                    cols[5].text.strip(),\n",
    "                    cols[6].text.strip(),\n",
    "                    cols[7].text.strip(),\n",
    "                    cols[8].text.strip(),\n",
    "                    cols[9].text.strip(),\n",
    "                    cols[10].text.strip(),\n",
    "                    cols[11].text.strip(),\n",
    "                    cols[12].text.strip(),\n",
    "                    cols[13].text.strip(),\n",
    "                    cols[14].text.strip(),\n",
    "                    cols[15].text.strip(),\n",
    "                    cols[16].text.strip(),\n",
    "                    cols[17].text.strip(),\n",
    "                    cols[18].text.strip(),\n",
    "                    cols[19].text.strip(),\n",
    "                    cols[20].text.strip(),\n",
    "                    cols[21].text.strip(),\n",
    "                    cols[22].text.strip(),\n",
    "                    cols[23].text.strip(),\n",
    "                    cols[24].text.strip(),\n",
    "                    cols[25].text.strip(),\n",
    "                    cols[26].text.strip(),\n",
    "                    cols[27].text.strip(),\n",
    "                    cols[28].text.strip(),\n",
    "                    cols[29].text.strip(),\n",
    "                    cols[30].text.strip(),\n",
    "                    cols[31].text.strip(),\n",
    "                    cols[32].text.strip()\n",
    "                ]\n",
    "                irow = irow+1\n",
    "        dd.index += 1\n",
    "         # Step 2: Sample of CSV being saved. Final Project will have all scraped data loaded in a mysql database\n",
    "        dd.to_csv(csvfile)\n",
    "    return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Clean the Data to Fit ERD\n",
    "def cleanStandardPitching(year):    \n",
    "    csvfile = str(year) + '-standard-pitching-cleaned.csv'\n",
    "    if os.path.isfile(csvfile):\n",
    "        df = pd.read_csv(csvfile)\n",
    "    else:\n",
    "        df = pd.read_csv(str(year) + '-standard-pitching.csv')\n",
    "        #remove unneeded columns\n",
    "        df = df.drop(columns=['Unnamed: 0','Age', 'Lg','W-L%','CG','BF','ERA+','FIP','WHIP','H9','BB9','SO9','SO/W'])\n",
    "        ## Rename columns\n",
    "        df=df.rename(columns={\"Name\": \"pitcher_name\", \"Tm\": \"team\",\"W\":\"wins\",\"L\":\"loses\",\"ERA\":\"era\",\"G\":\"g_played\",\"GS\":\"g_started\",\"GF\":\"g_finished\",\"SHO\":\"shutouts\",\"SV\":\"saves\",\"IP\":\"innings_pitched\", \"H\":\"hits\",\"R\":\"runs\",\"ER\":\"errors\",\"HR\":\"hr\",\"BB\":\"bb\",\"IBB\":\"int_bb\",\"SO\":\"strike_outs\",\"HBP\":\"hit_by_pitch\",\"BK\":\"balks\",\"WP\":\"wild\"})\n",
    "        ## add the year\n",
    "        df['year'] = str(year)\n",
    "        df.index += 1\n",
    "        df.to_csv(csvfile)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the Function to Get Data\n",
    "getStandardPitching(2015)\n",
    "getStandardPitching(2016)\n",
    "getStandardPitching(2017)\n",
    "getStandardPitching(2018)\n",
    "\n",
    "## Run the Fnciton to Clean Data \n",
    "pitchers_2015 = cleanStandardPitching(2015)\n",
    "pitchers_2016 = cleanStandardPitching(2016)\n",
    "pitchers_2017 = cleanStandardPitching(2017)\n",
    "pitchers_2018 = cleanStandardPitching(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge Data \n",
    "pitching_stats = pd.concat([pitchers_2015,pitchers_2016,pitchers_2017,pitchers_2018],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pitching_stats.drop(columns=['Unnamed: 0'])\n",
    "df.to_csv('pitching_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the Pitcher_Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a web scraping function\n",
    "def get_player_page_urls(url):    \n",
    "    #request the different URLS based on function input\n",
    "    url = requests.get(url,headers={'user-agent':'Mozilla/5.0'})\n",
    "    #pull the HTML code\n",
    "    soup = BeautifulSoup(url.content, \"html.parser\")\n",
    "    #Find the table we want to scrap\n",
    "    div = soup.find('div', id='all_players_standard_pitching')\n",
    "    #find the contents we want to scrap\n",
    "    bdiv = bytearray(str(div.contents),'utf-8')\n",
    "    #pull that contents into a soup object \n",
    "    soup2 = BeautifulSoup(bdiv, \"html.parser\")\n",
    "    tbody = soup2.find('tbody')\n",
    "    #find all the rows we want\n",
    "    rows = tbody.find_all('tr')\n",
    "    \n",
    "    #get all the urls for the players'personal info page \n",
    "    player_link=list()\n",
    "    player_name=list()\n",
    "    i=0\n",
    "    for row in rows:\n",
    "        i+=1\n",
    "        try:\n",
    "            str_row=str(row)\n",
    "            num_row = row.find('th').text\n",
    "            name = row.find('a').text\n",
    "            link = re.findall('<a.+?href=\"(/players/.+?)\">',str_row)\n",
    "            if i % 100 == 0:\n",
    "                print('saved',i,'page urls','now saving player:',name,\"...\")\n",
    "            player_link.append(link)\n",
    "            player_name.append(name)\n",
    "        except:\n",
    "            continue\n",
    "    return player_link,player_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [\"2015\",\"2016\",\"2017\",\"2018\"]\n",
    "all_player_link_list = []\n",
    "for year in years:\n",
    "    print('Saving year',year,'info...')\n",
    "    url = \"https://www.baseball-reference.com/leagues/MLB/\"+year+\"-standard-pitching.shtml\"\n",
    "    #begin scraping \n",
    "    #call the functions and find all player urls \n",
    "    player_link_list,player_name_list = get_player_page_urls(url)\n",
    "    all_player_link_list.append(player_link_list)\n",
    "print('Finish saving!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the urls into txt files\n",
    "f = open('all_player_link_list.txt','w')\n",
    "for link_list in all_player_link_list:\n",
    "    for i in range(len(link_list)):\n",
    "        f.write(link_list[i][0])\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the pitchers' information\n",
    "def get_player_info(url):\n",
    "    r=requests.get(url,{'user-agent':'Mozilla/5.0'})\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    #Find the info of items and save\n",
    "    info_list=[]\n",
    "    #Find the info of items and save\n",
    "    \n",
    "    info_text = soup.find('div',itemtype=\"https://schema.org/Person\").text.replace(u'\\n',u' ').replace(u'\\xa0',u' ').replace(u'\\t',u' ')\n",
    "\n",
    "    #=========== Find Name =======================================\n",
    "    try:\n",
    "        pitcher_name=soup.find('div','players').find('h1').text\n",
    "    except:\n",
    "        pitcher_name= 'N/A'\n",
    "\n",
    "\n",
    "    #=========== Find Height =======================================\n",
    "    try:\n",
    "\n",
    "        pitcher_height=soup.find('span',itemprop='height').text.replace('-','.')\n",
    "\n",
    "    except:\n",
    "        pitcher_height= 'NULL'\n",
    "\n",
    "\n",
    "    #=========== Find Weight=======================================\n",
    "    try:\n",
    "        pitcher_weight=soup.find('span',itemprop='weight').text.strip('lb')\n",
    "\n",
    "    except:\n",
    "        pitcher_weight= 'NULL'\n",
    "\n",
    "    #=========== Find birthday=======================================\n",
    "    try:\n",
    "        birthday=re.findall(r'data-birth=(.*)id',str(soup.find('span',itemprop='birthDate')))[0].replace('\"','')\n",
    "    except:\n",
    "        birthday= 'NULL'\n",
    "\n",
    "\n",
    "    #=========== Find Age=======================================\n",
    "    try:\n",
    "        now = datetime.now()\n",
    "        age = str(int(now.year) - int(birthday[:4]))\n",
    "    except:\n",
    "        age= 'NULL'    \n",
    "\n",
    "    #=========== Find Debut Age=======================================\n",
    "    try:\n",
    "        debut_age=re.findall(r'Age.?([0-9]{2}.?[0-9]{0,4}d)',info_text)[0]\n",
    "    except:\n",
    "        debut_age= 'NULL'\n",
    "\n",
    "    #=========== Find Birthplace =======================================\n",
    "    try:\n",
    "        birthplace =soup.find('span',itemprop='birthPlace').text.strip().replace(u'in',u'').replace(u'\\n',u'')\n",
    "    except:\n",
    "        birthplace= 'NULL'\n",
    "\n",
    "    #=========== ***** Find High School=======================================\n",
    "    try:\n",
    "        high_school=re.findall(r'High School: ([a-zA-Z_0-9 ]*)',info_text)[0].replace(u'\\n',u'').replace(u'\\t',u'')\n",
    "    except:\n",
    "        high_school= 'NULL'\n",
    "\n",
    "    #=========== ***** Find School=======================================\n",
    "    try:\n",
    "        school=re.findall(r'High School: .* School: ([a-zA-Z_0-9 ]*)',info_text)[0].replace(u'in',u'').replace(u'\\n',u'').replace(u'\\t',u'')\n",
    "        if school == high_school:\n",
    "                school = 'NULL'\n",
    "    except:\n",
    "        school= 'NULL'\n",
    "\n",
    "    #=========== Find Rookie Status=======================================\n",
    "    try:\n",
    "        Rookie_Status=re.findall(r'Rookie Status: ([\\-a-zA-Z_0-9 .]*)2020 Contract Status',info_text)[0].strip(' ')\n",
    "    except:\n",
    "        Rookie_Status= 'NULL'\n",
    "\n",
    "    # #=========== Find 2020 Contract Status=======================================\n",
    "    try:\n",
    "        Contract_Status=re.findall(r'2020 Contract Status: ([\\-a-zA-Z_0-9 .]*)Service Time',info_text)[0].strip(' ')\n",
    "    except:\n",
    "        Contract_Status= 'NULL'\n",
    "\n",
    "\n",
    "    # #=========== Find Arb Eligible=======================================\n",
    "    try:\n",
    "        Arb_Eligible=re.findall(r'Arb Eligible: ([a-zA-Z_0-9 ]*)',info_text)[0].strip(' ').replace(u'in',u'').replace(u'\\n',u'').replace(u'\\t',u'')\n",
    "    except:\n",
    "        Arb_Eligible= 'NULL'\n",
    "\n",
    "\n",
    "    # #=========== Find Free Agent=======================================\n",
    "    try:\n",
    "        Free_Agent=re.findall(r'Free Agent: ([a-zA-Z_0-9 ]*)Full Name:',info_text)[0].strip(' ').replace(u'in',u'').replace(u'\\n',u'').replace(u'\\t',u'')\n",
    "    except:\n",
    "        Free_Agent= 'NULL'\n",
    "\n",
    "    # #=========== Find Full Name=======================================\n",
    "    try:\n",
    "        Full_Name=re.findall(r'Full Name:</strong>([a-zA-Z_0-9 .]*)',str(soup.find('div',itemtype=\"https://schema.org/Person\")))[0].strip()\n",
    "    except:\n",
    "        Full_Name= 'NULL'\n",
    "\n",
    "\n",
    "    #save all info\n",
    "    info_list=[pitcher_name,pitcher_height,pitcher_weight,birthday,age,birthplace,\n",
    "               high_school,school,Rookie_Status,Contract_Status,\n",
    "               Arb_Eligible,Free_Agent,Full_Name]\n",
    "\n",
    "    return info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pitchers' information\n",
    "all_urls=[]\n",
    "f = open(\"all_player_link_list.txt\", \"r\")\n",
    "for line in f:\n",
    "    all_urls.append(line.strip('\\n'))\n",
    "\n",
    "all_info=[]\n",
    "i=0\n",
    "for link in all_urls:\n",
    "    i+=1\n",
    "    if i % 300 == 0:\n",
    "        print('Saving',i,'player info...')\n",
    "    url=\"https://www.baseball-reference.com\"+link\n",
    "    all_info.append(get_player_info(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label all the columns\n",
    "columns = ['Name','Height/ft','Weight/lb','Birthday','Age','Birthplace','High School','School','Rookie Status','2020 Contract Status',\n",
    "           'Arb Eligible','Free Agent','Full Name']\n",
    "#makes these the column names in the dataframe\n",
    "pitcher_info = pd.DataFrame(all_info,columns = columns)\n",
    "pitcher_info.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitcher_info.drop_duplicates(subset =\"Name\",keep = 'first', inplace = True)\n",
    "pitcher_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitcher_info.to_csv('pitcher_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Teams and Pitches table are from Kaggle dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b><font size=\"4\">Data Cleaning and Table Joining</font></b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the pitches table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the pitches table with abbats table\n",
    "pitches=pd.read_csv('pitches_acm.csv')pitches=pd.read_csv('pitches_acm.csv')\n",
    "ab=pd.read_csv('atbats.csv')\n",
    "pitches_ab=pitches.merge(right=ab, left_on='ab_id', right_on='ab_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns that can be use for analysis\n",
    "pitches_new=pitches_ab[['pitcher_id','batter_id','g_id','p_throws','inning','o','stand','top','end_speed','spin_rate','b_count','s_count','on_1b','on_2b','on_3b','pitch_num','pitch_type','code']]\n",
    "\n",
    "# Transform\n",
    "pitches_new.loc[pitches_new['top'] == True, 'top'] = 'top'\n",
    "pitches_new.loc[pitches_new['top'] == False, 'top'] = 'bottom'\n",
    "pitches_new.loc[pitches_new['on_1b'] == True, 'on_1b'] = 1\n",
    "pitches_new.loc[pitches_new['on_1b'] == False, 'on_1b'] = 0\n",
    "pitches_new.loc[pitches_new['on_2b'] == True, 'on_2b'] = 1\n",
    "pitches_new.loc[pitches_new['on_2b'] == False, 'on_2b'] = 0\n",
    "pitches_new.loc[pitches_new['on_3b'] == True, 'on_3b'] = 1\n",
    "pitches_new.loc[pitches_new['on_3b'] == False, 'on_3b'] = 0\n",
    "\n",
    "# Rename column names\n",
    "pitches_new=pitches_new.rename({'p_throws':'throw','o':'outs','top':'top_bottom','code':'outcome_code'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitches_new.to_csv(\"/Users/calvin/Desktop/Winter quarter/422/Final Project/mlb-pitch-data-20152018/Pitches.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the pitch_stats table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows with null value\n",
    "pitching_stats=pd.read_csv('Pitching_stats_v3.csv')\n",
    "pitching_stats = pitching_stats.dropna(subset=['pitcher_id'])\n",
    "\n",
    "pitching_stats.to_csv(\"/Users/calvin/Desktop/Winter quarter/422/Final Project/mlb-pitch-data-20152018/Pitching_stats.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the Batter_Info table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the batters' teams with their id and name\n",
    "batters=pd.read_csv('Batters.csv')\n",
    "batter_stats=pd.read_csv('batter_stats.csv')\n",
    "batter_stats.drop(batter_stats.columns[0],axis=1,inplace=True)\n",
    "\n",
    "team=pd.read_csv('Teams.csv')\n",
    "team_league=team[['team_abv','league']]\n",
    "team_id=team[['team_id','team_abv']]\n",
    "\n",
    "batter_withid=batter_stats.merge(right=batters,left_on='batter_name',right_on='batter_name')\n",
    "batter_withid=batter_withid.merge(right=team_league,left_on='team',right_on='team_abv')\n",
    "batter_withid.drop(batter_withid[['team_abv']],axis=1,inplace=True)\n",
    "\n",
    "batter_withid.to_csv(\"/Users/calvin/Desktop/Winter quarter/422/Final Project/mlb-pitch-data-20152018/Batter_Info.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b><font size=\"4\">Import csv files to MySQL database</font></b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Baseball database \n",
    "conn=pymysql.connect(host='localhost',user='root', passwd = '') \n",
    "cursor = conn.cursor()\n",
    "db_name = 'Baseball'\n",
    "cursor.execute('DROP DATABASE IF EXISTS %s' %db_name)\n",
    "cursor.execute('CREATE DATABASE IF NOT EXISTS %s' %db_name)\n",
    "cursor.execute('USE Baseball')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batter_Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Batter_Info table\n",
    "conn=pymysql.connect(host='localhost',user='root', passwd = '') \n",
    "cursor = conn.cursor()\n",
    "cursor.execute('USE Baseball')\n",
    "table_name = 'Batter_Info'\n",
    "cursor.execute('DROP TABLE IF EXISTS %s' %table_name)\n",
    "cursor.execute('CREATE TABLE %s(batter_id INT, batter_name varchar(30),team_id tinyint,year int)'%table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the batters csv file\n",
    "conn=pymysql.connect(host='localhost',user='root', passwd = '') \n",
    "cursor = conn.cursor()\n",
    "cursor.execute('USE Baseball')\n",
    "\n",
    "csv_data = csv.reader(open('Batter_Info.csv'))\n",
    "# execute and insert the csv into the database.\n",
    "next(csv_data) # Skip the header\n",
    "for row in csv_data:\n",
    "\tcursor.execute('INSERT INTO Batter_Info(batter_id,batter_name,team_id,year)''VALUES(%s,%s,%s,%s)',row)\n",
    "#close the connection to the database.\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "print(\"CSV has been imported into the database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Teams table\n",
    "conn=pymysql.connect(host='localhost',user='root', passwd = '') \n",
    "cursor = conn.cursor()\n",
    "cursor.execute('USE Baseball')\n",
    "table_name = 'Teams'\n",
    "cursor.execute('DROP TABLE IF EXISTS %s' %table_name)\n",
    "cursor.execute('CREATE TABLE %s(team_id int, team_abv varchar(5), team_name varchar(50),league varchar(20), region varchar(20))'%table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Teams csv file\n",
    "conn=pymysql.connect(host='localhost',user='root', passwd = '') \n",
    "cursor = conn.cursor()\n",
    "cursor.execute('USE Baseball')\n",
    "\n",
    "csv_data = csv.reader(open('Teams.csv'))\n",
    "# execute and insert the csv into the database.\n",
    "next(csv_data) # Skip the header\n",
    "for row in csv_data:\n",
    "\tcursor.execute('INSERT INTO Teams(team_id,team_abv,team_name,league,region)''VALUES(%s, %s, %s, %s, %s)',row)\n",
    "#close the connection to the database.\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "print(\"CSV has been imported into the database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pitches table\n",
    "conn=pymysql.connect(host='localhost',user='root', passwd = '') \n",
    "cursor = conn.cursor()\n",
    "cursor.execute('USE Baseball')\n",
    "table_name = 'Pitches'\n",
    "cursor.execute('DROP TABLE IF EXISTS %s' %table_name)\n",
    "cursor.execute('CREATE TABLE %s(pitcher_id int, batter_id int, g_id int,throw char(1), inning smallint, outs smallint, stand char(1), top_bottom char(10),end_speed float,spin_rate float,b_count tinyint,s_count tinyint,on_1b tinyint,on_2b tinyint,on_3b tinyint,pitch_num tinyint,pitch_type char(3),outcome_code char(2))'%table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Pitches csv file\n",
    "conn=pymysql.connect(host='localhost',user='root', passwd = '') \n",
    "cursor = conn.cursor()\n",
    "cursor.execute('USE Baseball')\n",
    "\n",
    "csv_data = csv.reader(open('Pitches.csv'))\n",
    "# execute and insert the csv into the database.\n",
    "next(csv_data) # Skip the header\n",
    "for row in csv_data:\n",
    "\tcursor.execute('INSERT INTO Pitches(pitcher_id,batter_id,g_id,throw,inning,outs,stand,top_bottom,end_speed,spin_rate,b_count,s_count,on_1b,on_2b,on_3b,pitch_num,pitch_type,outcome_code)''VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s,%s, %s, %s, %s, %s, %s, %s, %s)',row)\n",
    "#close the connection to the database.\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "print(\"CSV has been imported into the database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Due to the size of the dataset, we didn't use the code to import the file. Instead we use the import wizard tool in the MySQLWorkbench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pitcher_Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pitching_Stats table\n",
    "conn=pymysql.connect(host='localhost',user='root', passwd = '') \n",
    "cursor = conn.cursor()\n",
    "cursor.execute('USE Baseball')\n",
    "table_name = 'Pitching_Stats'\n",
    "cursor.execute('DROP TABLE IF EXISTS %s' %table_name)\n",
    "cursor.execute('CREATE TABLE %s(stats_id int,pitcher_id int,team_id tinyint,wins tinyint,loses tinyint,era float,g_played smallint,g_started smallint,g_finished smallint,shutouts smallint,saves smallint,innings_pitched float,hits smallint,runs smallint,errors smallint,hr tinyint,bb tinyint,int_bb tinyint,strike_outs smallint,hit_by_pitch tinyint,balks tinyint,wild tinyint,year int)'%table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Pitching_Stats csv file\n",
    "conn=pymysql.connect(host='localhost',user='root', passwd = '') \n",
    "cursor = conn.cursor()\n",
    "cursor.execute('USE Baseball')\n",
    "\n",
    "csv_data = csv.reader(open('Pitching_Stats.csv'))\n",
    "# execute and insert the csv into the database.\n",
    "next(csv_data) # Skip the header\n",
    "for row in csv_data:\n",
    "\tcursor.execute('INSERT INTO Pitching_Stats(stats_id,pitcher_id,team_id,wins,loses,era,g_played,g_started,g_finished,shutouts,saves,innings_pitched,hits,runs,errors,hr,bb,int_bb,strike_outs,hit_by_pitch,balks,wild,year)''VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s,%s, %s, %s, %s, %s, %s, %s, %s, %s, %s,%s, %s, %s,%s)',row)\n",
    "#close the connection to the database.\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "print(\"CSV has been imported into the database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Due to the size of the dataset, we didn't use the code to import the file. Instead we use the import wizard tool in the MySQLWorkbench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pitcher_Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pitcher_Info table\n",
    "conn=pymysql.connect(host='localhost',user='root', passwd = '') \n",
    "cursor = conn.cursor()\n",
    "cursor.execute('USE Baseball')\n",
    "table_name = 'Pitcher_Info'\n",
    "cursor.execute('DROP TABLE IF EXISTS %s' %table_name)\n",
    "cursor.execute('CREATE TABLE %s(pitcher_id int,pitcher_name varchar(20),Height/ft float,Weight/lb tinyint,Birthday char(10),Age tinyint,Birthplace varchar(50),High_School varchar(50),School varchar(50),Rookie_Status varchar(50),2020_Contract_Status varchar(20),Arb_Eligible int,Free_Agent int,Full_Name varchar(50))'%table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Pitcher_Info csv file\n",
    "conn=pymysql.connect(host='localhost',user='root', passwd = '') \n",
    "cursor = conn.cursor()\n",
    "cursor.execute('USE Baseball')\n",
    "\n",
    "csv_data = csv.reader(open('Pitcher_Info.csv'))\n",
    "# execute and insert the csv into the database.\n",
    "next(csv_data) # Skip the header\n",
    "for row in csv_data:\n",
    "\tcursor.execute('INSERT INTO Pitcher_Info(pitcher_id,pitcher_name,height/ft,weight/lb,birthday,age,school,full_Name)''VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)',row)\n",
    "#close the connection to the database.\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "print(\"CSV has been imported into the database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Due to the size of the dataset, we didn't use the code to import the file. Instead we use the import wizard tool in the MySQLWorkbench."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
